---
layout: post
title: "SGAE"
subtitle: "Auto-Encoding Scene Graphs for Image Captioning"
date: 2021-01-08
author: Chen JingXiang
lang: zh
key: 'p2021010801'
enable_comm: true
mathjax: true
tags:
    - 论文笔记
    - Image Caption
---

### 1. 摘要

+ 本文提出了一种名为Scene Graph Auto-Encoder（SGAE）的方法生成图像描述，它主要是结合语言归纳偏置（language inductive bias）使得生成的文本更加接近人类的表述。直觉上，人们在话语中使用归纳偏置来组合搭配和上下文推理。例如，当我们看到“person on bike”很自然地会替换on为ride，并推理“人在路上骑车”，甚至路是不明显的。因此利用这种偏置作为语言先验知识来帮助传统的编码-解码模型更少地去拟合数据集，而是去集中于推理。具体来说，我们使用场景图（scene graph）- 一个有向图，目标结点连接着属性结点和关系结点 - 来表示图像和句子的复杂结构。在文本领域，我们使用SGAE学习到一个字典D，在S-G-D-S通道上帮助重构句子，其中D是编码了想要的语言归纳偏置。在视觉-文本领域，我们使用共享的D在I-G-D-S通道来指导解码器-编码器。由于场景图表示和共享字典，归纳偏差原则上跨域转移。我们在MS-COCO image captioning benchmark验证了SGAE的有效性，基于SGAE的单模型实现了最高的127.8CIDEr-D(Karpathy split)，在官方服务器上的125.5CIDEr-D(c40)甚至比其他集成模型都有竞争力。

### 2. 贡献
1. 提出了SGAE(Scene graph Auto-Encoder)学习语言归纳偏置（language inductive bias）。
2. 提出了多模态图卷积神经网（multi-modal graph convolutional network）络将场景图转化为视觉特征。
3. 提出了基于SGAE的带共享字典的编码解码器。

### 3. 整体框架

![20210108145453](/img/im-post/sgae/20210108145453.png)

![20210108145527](/img/im-post/sgae/20210108145527.png)

### 4. SGAE: $S \rightarrow G \rightarrow D \rightarrow S$

<img src="/img/im-post/sgae/20210108145548.png" alt="20210108145548"/>

 1.  $S \rightarrow G$: scene graph parser

     + 每一个节点表示$o_i,a_{il},r_{ij}$
       
     + 对于G中的每一个节点都表示为d维的向量$e_o,e_r,e_a$
       
     + 细节：对于MSCOCO中的每一句话，使用scene graph parser解析得到objects、attributes、realtionships三种结点，过滤掉所有出现次数少于10次的objects、attributes、realtionships结点，则剩下5364个objects类，1308个realtionships类，3430个attributes类。三种label一共10102个，所以one-hot向量也是10102维，然后再乘上词嵌入向量$W_{\sum S}$，得到$e_o,e_r,e_a$。
 2.  $G \rightarrow X$: spatial graph convolutions
     + 相同结构，独立参数的网络：向量连接输入到全连接层，后面跟着一个ReLU。
     + 关系节点：
       + $x_{r_{ij}} = g_r(e_o,e_r,e_a)$
     + 属性节点：
       + $x_{a_i} = \frac{1}{Na_i}\sum^{Na_i}_{l=1} g_a(e_{o_i},e_{a_{i,l}})$
       + ai表示目标oi的所有属性节点
     + 目标节点：
       + $x_{o_i} = \frac{1}{Nr_i}(\sum_{o_j\in sub(o_i)} g_s(e_{o_i},e_{o_j},e_{r_{ij}})+\sum_{o_k\in obj(o_i)} g_s(e_{o_k},e_{o_i},e_{r_{ki}}))$
       + oj表示所有oi作为主语所连接的目标节点
       + ok表示所有oi作为宾语所连接的目标节点

 3.  $R(X;D) \rightarrow \hat X$:![20210108145559](/img/im-post/sgae/20210108145559.png)
+ a working memory记忆网络存储归纳偏置到D
    + x和D中的每一个d做内积，计算权重α
+ $\hat x = R(x;D) = D\alpha = \sum^{K}_{k=1}\alpha_k d_k$
      	+ $\alpha = softmax(D^Tx)$
      + α和D加权求和
  4.  $\hat X \rightarrow S$：trainable RNN-based language decoder双层LSMT，输入的是D的输出

### 5. 基于SGAE的编码解码模型：$I \rightarrow G \rightarrow D \rightarrow S$

![20210108145539](/img/im-post/sgae/20210108145539.png)

1. $I \rightarrow V$
   		+ Faster-RCNN目标检测，得到目标特征V
 2. $V \rightarrow G$
    	+ MOTIFS关系检测器和属性分类器，得到目标的属性和关系。
    + 编码：融合语义标签特征e和视觉特征v，得到u，场景图每个结点的特征。
3. $G \rightarrow V^`$
    + MGCN，这里的多模态的意思是对融合特征u进行编码，得到$V^`$
    + 公式和GCN一样，这里得到的是$V^`$
4. $R(V^`,G; D)→ \hat V$
    + $\hat V=D\alpha$
    + $\alpha = softmax(D^ TV^`)$
    + 经过编码的场景图经过字典D，重新编码。
5. $\hat V \rightarrow S$
   + 注意，这里输入LSTM的$\hat V$和$V^`$的拼接向量，得到句子S。

### 6. 训练过程

1. 首先使用交叉熵损失函数训练 S->G->S 20 epoch。注意D不参与训练
   + 输入MSCOCO中的sentence，通过S-G-S的流程重构S。

2. 使用交叉熵损失函数训练 S->G->D->S 20 epoch。这里的D参与训练
	+ 输入MSCOCO中的sentence，通过S-G-D-S的流程重构S，训练D。
3. 使用交叉熵损失函数训练 I->G->D->S 20 epoch。这里的D使用在上一步骤中预训练的参数，并fine-tune
	+ 输入MSCOCO中的image，通过I-G-D-S训练模型，交叉熵损失。
4. 使用RL-based reward 训练 I->G->D->S 40 epoch。 D参与训练
	+ 输入MSCOCO中的image，通过I-G-D-S训练模型，RL-based reward。

### 7. 总结
+ 提出了SGAE学习语言归纳偏置，就是通过重构输入的句子学习得到一个归纳偏置字典D。然后提取图像的场景图，利用MGCN对场景图进行编码，经过字典D重新编码，得到的向量输入双层LSTM中生成句子。而在S->G->D->S中，输入的句子是由人类生成的，因此字典D中就保留有人类的归纳偏置。进而与I->G->D->S共享，使得由图像生成的描述也含有人类的归纳偏置。

+ 这里的归纳偏置用通俗的语言讲就是一种先验知识。
+ 问题1：这个D是怎么训练的？为什么会起作用？